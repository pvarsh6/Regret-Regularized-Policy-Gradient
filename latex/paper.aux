\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{freund1999adaptive}
\citation{auer2002nonstochastic}
\citation{hart2000simple}
\citation{sutton1999policy,schulman2017proximal}
\citation{arora2012multiplicative}
\citation{zinkevich2007regret}
\citation{auer2002nonstochastic}
\citation{williams1992simple}
\newlabel{eq:rm_policy}{{4}{3}{}{equation.4}{}}
\newlabel{eq:rm_policy@cref}{{[equation][4][]4}{[1][2][]3}{}{}{}}
\newlabel{sec:method}{{3}{3}{}{section.3}{}}
\newlabel{sec:method@cref}{{[section][3][]3}{[1][3][]3}{}{}{}}
\citation{kingma2014adam}
\newlabel{alg:rrpg}{{1}{4}{}{algorithm.1}{}}
\newlabel{alg:rrpg@cref}{{[algorithm][1][]1}{[1][4][]4}{}{}{}}
\newlabel{sec:variants}{{3.7}{5}{}{subsection.3.7}{}}
\newlabel{sec:variants@cref}{{[subsection][7][3]3.7}{[1][5][]5}{}{}{}}
\newlabel{sec:results}{{5}{7}{}{section.5}{}}
\newlabel{sec:results@cref}{{[section][5][]5}{[1][7][]7}{}{}{}}
\newlabel{sec:results:regret}{{5.1}{7}{}{subsection.5.1}{}}
\newlabel{sec:results:regret@cref}{{[subsection][1][5]5.1}{[1][7][]7}{}{}{}}
\newlabel{tab:adaptive_summary}{{1}{7}{Performance against adaptive opponents (final evaluation window: last 1000 rounds; mean $\pm $ std over 20 seeds). ``Ext. regret'' is mean per-round external regret in the final window. Exploitability is reported only for zero-sum games. Lower is better for both metrics}{table.1}{}}
\newlabel{tab:adaptive_summary@cref}{{[table][1][]1}{[1][7][]7}{}{}{}}
\newlabel{sec:results:static}{{5.1.2}{7}{}{subsubsection.5.1.2}{}}
\newlabel{sec:results:static@cref}{{[subsubsection][2][5,1]5.1.2}{[1][7][]7}{}{}{}}
\newlabel{tab:static_summary}{{2}{7}{Performance against static opponents (final evaluation window: last 1000 rounds). ``Ext. regret'' is mean per-round external regret in the final window. All methods eventually exploit static strategies}{table.2}{}}
\newlabel{tab:static_summary@cref}{{[table][2][]2}{[1][7][]7}{}{}{}}
\newlabel{tab:entropy_comparison}{{3}{8}{Policy entropy (final evaluation window: last 1000 rounds) for Matching Pennies vs MW. Higher entropy indicates more uniform (less exploitable) policies}{table.3}{}}
\newlabel{tab:entropy_comparison@cref}{{[table][3][]3}{[1][8][]8}{}{}{}}
\newlabel{sec:results:strategic}{{5.2}{8}{}{subsection.5.2}{}}
\newlabel{sec:results:strategic@cref}{{[subsection][2][5]5.2}{[1][8][]8}{}{}{}}
\newlabel{sec:ablations}{{6}{8}{}{section.6}{}}
\newlabel{sec:ablations@cref}{{[section][6][]6}{[1][8][]8}{}{}{}}
\newlabel{sec:ablations:lambda}{{6.1}{8}{}{subsection.6.1}{}}
\newlabel{sec:ablations:lambda@cref}{{[subsection][1][6]6.1}{[1][8][]8}{}{}{}}
\newlabel{tab:lambda_sweep}{{4}{8}{Effect of regret-matching coefficient $\lambda _R$ (Matching Pennies vs. MW, final evaluation window: last 1000 rounds)}{table.4}{}}
\newlabel{tab:lambda_sweep@cref}{{[table][4][]4}{[1][8][]8}{}{}{}}
\newlabel{tab:variants}{{5}{9}{Functional form variants (Matching Pennies vs. MW, final evaluation window: last 1000 rounds)}{table.5}{}}
\newlabel{tab:variants@cref}{{[table][5][]5}{[1][8][]9}{}{}{}}
\citation{hart2000simple}
\citation{bubeck2015convex}
\citation{freund1999adaptive,auer2002nonstochastic,hart2000simple}
\citation{zinkevich2007regret}
\citation{bowling2015heads}
\citation{sutton1999policy,williams1992simple,schulman2017proximal}
\citation{gleave2020adversarial}
\citation{silver2016mastering}
\citation{berner2019dota}
\citation{haarnoja2018soft}
\citation{schulman2017proximal}
\citation{brown2019superhuman}
\citation{heinrich2015fictitious}
\citation{steinberger2019single}
\bibdata{paper}
\bibcite{arora2012multiplicative}{{1}{2012}{{Arora et~al.}}{{Arora, Hazan, and Kale}}}
\bibcite{auer2002nonstochastic}{{2}{2002}{{Auer et~al.}}{{Auer, Cesa-Bianchi, Freund, and Schapire}}}
\bibcite{berner2019dota}{{3}{2019}{{Berner et~al.}}{{Berner, Brockman, Chan, Cheung, D\k {e}biak, Dennison, Farhi, Fischer, Hashme, Hesse, et~al.}}}
\bibcite{bowling2015heads}{{4}{2015}{{Bowling et~al.}}{{Bowling, Burch, Johanson, and Tammelin}}}
\bibcite{brown2019superhuman}{{5}{2019}{{Brown \& Sandholm}}{{Brown and Sandholm}}}
\bibcite{bubeck2015convex}{{6}{2015}{{Bubeck}}{{}}}
\bibcite{freund1999adaptive}{{7}{1999}{{Freund \& Schapire}}{{Freund and Schapire}}}
\bibcite{gleave2020adversarial}{{8}{2020}{{Gleave et~al.}}{{Gleave, Dennis, Wild, Kant, Levine, and Russell}}}
\bibcite{haarnoja2018soft}{{9}{2018}{{Haarnoja et~al.}}{{Haarnoja, Zhou, Abbeel, and Levine}}}
\bibcite{hart2000simple}{{10}{2000}{{Hart \& Mas-Colell}}{{Hart and Mas-Colell}}}
\bibcite{heinrich2015fictitious}{{11}{2016}{{Heinrich \& Silver}}{{Heinrich and Silver}}}
\bibcite{kingma2014adam}{{12}{2014}{{Kingma \& Ba}}{{Kingma and Ba}}}
\bibcite{schulman2017proximal}{{13}{2017}{{Schulman et~al.}}{{Schulman, Wolski, Dhariwal, Radford, and Klimov}}}
\bibcite{silver2016mastering}{{14}{2016}{{Silver et~al.}}{{Silver, Huang, Maddison, Guez, Sifre, Van Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, et~al.}}}
\bibcite{steinberger2019single}{{15}{2019}{{Steinberger}}{{}}}
\bibcite{sutton1999policy}{{16}{1999}{{Sutton et~al.}}{{Sutton, McAllester, Singh, and Mansour}}}
\bibcite{williams1992simple}{{17}{1992}{{Williams}}{{}}}
\bibcite{zinkevich2007regret}{{18}{2007}{{Zinkevich et~al.}}{{Zinkevich, Johanson, Bowling, and Piccione}}}
\bibstyle{icml2025}
\newlabel{app:additional}{{A}{14}{Impact Statement}{appendix.A}{}}
\newlabel{app:additional@cref}{{[section][1][]A}{[1][14][]14}{}{}{}}
\newlabel{app:performance}{{A.1}{15}{Impact Statement}{subsection.A.1}{}}
\newlabel{app:performance@cref}{{[subsection][1][1]A.1}{[1][14][]15}{}{}{}}
\newlabel{app:adaptive_mp_metrics}{{A.1.1}{15}{Impact Statement}{subsubsection.A.1.1}{}}
\newlabel{app:adaptive_mp_metrics@cref}{{[subsubsection][1][1,1]A.1.1}{[1][14][]15}{}{}{}}
\newlabel{fig:mp_adaptive_complete}{{1}{15}{Complete performance metrics for Matching Pennies against adaptive opponents (MW and EXP3). From top to bottom: (a) External regret, (b) Exploitability, (c) Instant regret. All 18 algorithms shown with 95\% confidence intervals over 20 seeds}{figure.1}{}}
\newlabel{fig:mp_adaptive_complete@cref}{{[figure][1][]1}{[1][14][]15}{}{}{}}
\newlabel{app:adaptive_rps_metrics}{{A.1.2}{16}{Impact Statement}{subsubsection.A.1.2}{}}
\newlabel{app:adaptive_rps_metrics@cref}{{[subsubsection][2][1,1]A.1.2}{[1][16][]16}{}{}{}}
\newlabel{fig:rps_adaptive_complete}{{2}{16}{Complete performance metrics for Rock-Paper-Scissors against adaptive opponents. From top to bottom: (a) External regret, (b) Exploitability, (c) Instant regret. RRPG variants consistently achieve lower regret and exploitability}{figure.2}{}}
\newlabel{fig:rps_adaptive_complete@cref}{{[figure][2][]2}{[1][16][]16}{}{}{}}
\newlabel{app:static_mp_metrics}{{A.1.3}{17}{Impact Statement}{subsubsection.A.1.3}{}}
\newlabel{app:static_mp_metrics@cref}{{[subsubsection][3][1,1]A.1.3}{[1][17][]17}{}{}{}}
\newlabel{fig:mp_static_complete}{{3}{17}{Complete performance metrics for Matching Pennies against static opponents (Deterministic, Biased, Uniform). From top to bottom: (a) External regret, (b) Exploitability, (c) Instant regret. All learning algorithms eventually exploit static strategies}{figure.3}{}}
\newlabel{fig:mp_static_complete@cref}{{[figure][3][]3}{[1][17][]17}{}{}{}}
\newlabel{app:static_rps_metrics}{{A.1.4}{18}{Impact Statement}{subsubsection.A.1.4}{}}
\newlabel{app:static_rps_metrics@cref}{{[subsubsection][4][1,1]A.1.4}{[1][18][]18}{}{}{}}
\newlabel{fig:rps_static_complete}{{4}{18}{Complete performance metrics for Rock-Paper-Scissors against static opponents. From top to bottom: (a) External regret, (b) Exploitability, (c) Instant regret. Regret regularization does not impair performance in non-adaptive settings}{figure.4}{}}
\newlabel{fig:rps_static_complete@cref}{{[figure][4][]4}{[1][18][]18}{}{}{}}
\newlabel{app:ablations}{{A.2}{19}{Impact Statement}{subsection.A.2}{}}
\newlabel{app:ablations@cref}{{[subsection][2][1]A.2}{[1][19][]19}{}{}{}}
\newlabel{app:lambda_sweep}{{A.2.1}{19}{Impact Statement}{subsubsection.A.2.1}{}}
\newlabel{app:lambda_sweep@cref}{{[subsubsection][1][1,2]A.2.1}{[1][19][]19}{}{}{}}
\newlabel{fig:lambda_sweep}{{5}{19}{Effect of regret-matching coefficient $\lambda _R \in \{0.05, 0.1, 0.2, 0.5, 1.0, 2.0\}$ on average external regret over time. Top: Matching Pennies vs MW. Bottom: Rock-Paper-Scissors vs MW. All values in $[0.1, 2.0]$ substantially outperform vanilla PG ($\lambda _R = 0$). Stronger regularization ($\lambda _R = 2.0$) performs best at our 10,000-round horizon}{figure.5}{}}
\newlabel{fig:lambda_sweep@cref}{{[figure][5][]5}{[1][19][]19}{}{}{}}
\newlabel{app:functional_forms}{{A.2.2}{20}{Impact Statement}{subsubsection.A.2.2}{}}
\newlabel{app:functional_forms@cref}{{[subsubsection][2][1,2]A.2.2}{[1][20][]20}{}{}{}}
\newlabel{fig:functional_forms_mw}{{6}{20}{Comparison of functional form variants against MW opponent. Top: Matching Pennies. Bottom: Rock-Paper-Scissors. Variants shown: RM+ (standard clipping), RM-NoClip (shift to non-negative), and Softmax-RM with $\tau \in \{0.5, 1.0\}$. Standard RM+ achieves lowest regret, suggesting the clipping operation provides useful inductive bias}{figure.6}{}}
\newlabel{fig:functional_forms_mw@cref}{{[figure][6][]6}{[1][20][]20}{}{}{}}
\newlabel{fig:functional_forms_exp3}{{7}{21}{Functional form comparison against EXP3 opponent. Top: Matching Pennies. Bottom: Rock-Paper-Scissors. Results are consistent across opponent types, with RM+ clipping providing best performance}{figure.7}{}}
\newlabel{fig:functional_forms_exp3@cref}{{[figure][7][]7}{[1][21][]21}{}{}{}}
\newlabel{app:discounting}{{A.2.3}{22}{Impact Statement}{subsubsection.A.2.3}{}}
\newlabel{app:discounting@cref}{{[subsubsection][3][1,2]A.2.3}{[1][22][]22}{}{}{}}
\newlabel{fig:discounting}{{8}{22}{Comparison of cumulative vs discounted regret with $\gamma \in \{0.95, 0.99\}$. Top: Matching Pennies vs MW. Bottom: Rock-Paper-Scissors vs MW. Discounting achieves notably lower regret, suggesting benefit from down-weighting early-game noise when policies are far from optimal}{figure.8}{}}
\newlabel{fig:discounting@cref}{{[figure][8][]8}{[1][22][]22}{}{}{}}
\newlabel{app:schedules}{{A.2.4}{23}{Impact Statement}{subsubsection.A.2.4}{}}
\newlabel{app:schedules@cref}{{[subsubsection][4][1,2]A.2.4}{[1][23][]23}{}{}{}}
\newlabel{fig:schedules}{{9}{23}{Comparison of constant $\lambda _R = 2.0$ vs progressive decay schedules (linear and exponential). Top: Matching Pennies vs MW. Bottom: Rock-Paper-Scissors vs MW. Constant regularization performs best in our stationary setting, but schedules may be beneficial in non-stationary environments}{figure.9}{}}
\newlabel{fig:schedules@cref}{{[figure][9][]9}{[1][23][]23}{}{}{}}
\newlabel{app:components}{{A.2.5}{24}{Impact Statement}{subsubsection.A.2.5}{}}
\newlabel{app:components@cref}{{[subsubsection][5][1,2]A.2.5}{[1][24][]24}{}{}{}}
\newlabel{fig:component_ablation_mw}{{10}{24}{Ablation isolating regret regularization from entropy regularization against MW opponent. Top: Matching Pennies. Bottom: Rock-Paper-Scissors. RM+only ($\lambda _R = 2.0, \lambda _H = 0$) achieves very low regret, while Entropyonly ($\lambda _R = 0, \lambda _H = 0.1$) performs poorly. This confirms regret-matching regularization is the key driver of performance improvement}{figure.10}{}}
\newlabel{fig:component_ablation_mw@cref}{{[figure][10][]10}{[1][24][]24}{}{}{}}
\newlabel{fig:component_ablation_exp3}{{11}{25}{Component ablation against EXP3 opponent. Top: Matching Pennies. Bottom: Rock-Paper-Scissors. Results confirm that regret regularization provides qualitatively different signal than generic exploration bonuses}{figure.11}{}}
\newlabel{fig:component_ablation_exp3@cref}{{[figure][11][]11}{[1][25][]25}{}{}{}}
\newlabel{app:baselines}{{A.3}{26}{Impact Statement}{subsection.A.3}{}}
\newlabel{app:baselines@cref}{{[subsection][3][1]A.3}{[1][26][]26}{}{}{}}
\newlabel{fig:baseline_comparison_mw}{{12}{26}{Baseline comparison against MW opponent. Top: Matching Pennies. Bottom: Rock-Paper-Scissors. Shows MW, EXP3, vanilla PG with/without entropy, and RRPG variants. Classical no-regret algorithms (MW, EXP3) achieve near-zero regret as expected. RRPG substantially narrows the gap between vanilla neural policy gradient and classical methods}{figure.12}{}}
\newlabel{fig:baseline_comparison_mw@cref}{{[figure][12][]12}{[1][26][]26}{}{}{}}
\newlabel{fig:baseline_comparison_exp3}{{13}{27}{Baseline comparison against EXP3 opponent. Top: Matching Pennies. Bottom: Rock-Paper-Scissors. Comprehensive comparison showing all baseline algorithms and RRPG variants}{figure.13}{}}
\newlabel{fig:baseline_comparison_exp3@cref}{{[figure][13][]13}{[1][27][]27}{}{}{}}
\newlabel{app:static_dynamics}{{A.4}{28}{Impact Statement}{subsection.A.4}{}}
\newlabel{app:static_dynamics@cref}{{[subsection][4][1]A.4}{[1][28][]28}{}{}{}}
\newlabel{fig:static_dynamics_mp}{{14}{28}{Learning dynamics for Matching Pennies against static opponents. From top to bottom: (a) Deterministic opponent, (b) Biased opponent (60\% Heads), (c) Uniform random opponent. All methods eventually exploit deterministic and biased opponents. Against uniform random, all methods converge to near-zero regret}{figure.14}{}}
\newlabel{fig:static_dynamics_mp@cref}{{[figure][14][]14}{[1][28][]28}{}{}{}}
\newlabel{fig:static_dynamics_rps}{{15}{29}{Learning dynamics for Rock-Paper-Scissors against static opponents. From top to bottom: (a) Deterministic opponent, (b) Biased opponent, (c) Uniform random opponent. Consistent with Matching Pennies, regret regularization does not impair exploitation of fixed strategies}{figure.15}{}}
\newlabel{fig:static_dynamics_rps@cref}{{[figure][15][]15}{[1][29][]29}{}{}{}}
\newlabel{app:diagnostics}{{A.5}{30}{Impact Statement}{subsection.A.5}{}}
\newlabel{app:diagnostics@cref}{{[subsection][5][1]A.5}{[1][30][]30}{}{}{}}
\newlabel{app:diagnostics:cum_regret}{{A.5.1}{30}{Impact Statement}{subsubsection.A.5.1}{}}
\newlabel{app:diagnostics:cum_regret@cref}{{[subsubsection][1][1,5]A.5.1}{[1][30][]30}{}{}{}}
\newlabel{fig:cumulative_regret}{{16}{30}{Cumulative external regret $R_T$ over time against MW opponent. Top: Matching Pennies. Bottom: Rock-Paper-Scissors. RRPG achieves sublinear growth similar to classical no-regret algorithms, while vanilla PG exhibits linear growth}{figure.16}{}}
\newlabel{fig:cumulative_regret@cref}{{[figure][16][]16}{[1][30][]30}{}{}{}}
\newlabel{app:diagnostics:instant_regret}{{A.5.2}{31}{Impact Statement}{subsubsection.A.5.2}{}}
\newlabel{app:diagnostics:instant_regret@cref}{{[subsubsection][2][1,5]A.5.2}{[1][31][]31}{}{}{}}
\newlabel{fig:instant_regret}{{17}{31}{Average cumulative instant regret (regret of best response at each round) against MW opponent. Top: Matching Pennies. Bottom: Rock-Paper-Scissors. This represents an oracle upper bound on achievable performance. All learning algorithms perform below this bound as expected}{figure.17}{}}
\newlabel{fig:instant_regret@cref}{{[figure][17][]17}{[1][31][]31}{}{}{}}
\gdef \@abspage@last{31}
